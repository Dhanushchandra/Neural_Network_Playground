{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5976a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain langchain-community langchain-core langchain-classic langchain_chroma langchain-huggingface langchain-google-genai langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa869fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_classic.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a33b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a98bc3ec68468f996ab96eeae8d5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a62f82c03c42f9835e1a23e6fccee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f09584a8b6942429141f49c096832cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a56ea8dfc8a4d81beaac51de5cb64c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318df1df1f45478286a42ded62588e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cdb0cfecd0497ab42b9574b8cf0983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a764f43fd847478bfb4aac3bf2ac9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29d99e8bebc4a06bfaecbed395fabbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642c112d1e0a4c15b849b6bae86eabdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052ea9fdc7b243828c8909edcf0900c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6223f1ddd59d4d2da9ec08259ea231c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945f8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma(\n",
    "    persist_directory=\"my_vectordb\", \n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"office_docs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f67639",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e6a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f743b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c10a14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood, \"\n",
    "    \"If user can answer the user Reminder points\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd3b5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"), # Dynamic history goes here\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5d0c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abd9193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60210325",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0896848",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5186f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41bb5aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    \n",
    "    # TRIMMING: Keep only last 4 messages to stay efficient\n",
    "    if len(store[session_id].messages) > 4:\n",
    "        store[session_id].messages = store[session_id].messages[-4:]\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a9059d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "411ed562",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_1_config = {\"configurable\": {\"session_id\": \"user_123\"}}\n",
    "user_2_config = {\"configurable\": {\"session_id\": \"user_124\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fbb5561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1: What is the office location?\n",
      "AI: Our office is located in Bangalore.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res1 = conversational_rag_chain.invoke({\"input\": \"What is the office location?\"}, config=user_1_config)\n",
    "print(f\"User 1: What is the office location?\\nAI: {res1['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08ebf072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1: What country is that city in?\n",
      "AI: Bangalore is a city in India.\n"
     ]
    }
   ],
   "source": [
    "res2 = conversational_rag_chain.invoke({\"input\": \"What country is that city in?\"}, config=user_1_config)\n",
    "print(f\"User 1: What country is that city in?\\nAI: {res2['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb5d9a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1: Remember this Number\n",
      "AI: I've remembered the number: 3.\n"
     ]
    }
   ],
   "source": [
    "res3 = conversational_rag_chain.invoke({\"input\": \"Remember this Number - 3\"}, config=user_1_config)\n",
    "print(f\"User 1: Remember this Number\\nAI: {res3['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba634df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1: What number I asked you to remember?\n",
      "AI: You asked me to remember the number 3.\n"
     ]
    }
   ],
   "source": [
    "res4 = conversational_rag_chain.invoke({\"input\": \"What number I asked you to remember?\"}, config=user_1_config)\n",
    "print(f\"User 1: What number I asked you to remember?\\nAI: {res4['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0be359c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 2: What number I asked you to remember?\n",
      "AI: You didn't ask me to remember any number. Our conversation just started with you providing some context about our office being in Bangalore and the CEO being Sarah Jenkins.\n"
     ]
    }
   ],
   "source": [
    "res5 = conversational_rag_chain.invoke({\"input\": \"What number I asked you to remember?\"}, config=user_2_config)\n",
    "print(f\"User 2: What number I asked you to remember?\\nAI: {res5['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acafd142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What country is that city in?', additional_kwargs={}, response_metadata={}), AIMessage(content='Bangalore is a city in India.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='Remember this Number - 3', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I've remembered the number: 3.\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='What number I asked you to remember?', additional_kwargs={}, response_metadata={}), AIMessage(content='You asked me to remember the number 3.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]),\n",
       " 'user_124': InMemoryChatMessageHistory(messages=[HumanMessage(content='What number I asked you to remember?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You didn't ask me to remember any number. Our conversation just started with you providing some context about our office being in Bangalore and the CEO being Sarah Jenkins.\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
